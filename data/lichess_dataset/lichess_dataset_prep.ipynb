{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "#%load_ext cython\n",
    "\n",
    "PATH_DATASET_JSONL = \"data\\\\lichess_db_eval.jsonl\" # this is huge\n",
    "PATH_DATASET_JSONL_REDUCED = \"data\\\\lichess_db_eval_only_fen_and_deepest_best_line.jsonl\" # only the FEN and the best line with its eval\n",
    "PATH_DATASET_JSONL_REDUCED_WIN_PROB = \"data\\\\lichess_db_eval_fen_and_win_probability.jsonl\" # only the FEN and the win probability\n",
    "PATH_DATASET_JSONL_FEN_WIN_LINE = \"data\\\\lichess_db_eval_fen_win_line.jsonl\" # only the FEN, win probability and the best line\n",
    "PATH_DIR_DATASET_JSONL_REDUCED_WIN_PROB_SPLIT = \"data\\\\lichess_db_eval_fen_and_win_probability_split\" # split the win probability dataset into separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq -\n",
      "len(evals): 4\n",
      "{'pvs': [{'cp': 21, 'line': 'd2d4 g8f6 c2c4 e7e6 g1f3 d7d5 g2g3 f8b4 c1d2 b4e7'}, {'cp': 20, 'line': 'e2e4 e7e5 g1f3 b8c6 f1c4 g8f6 d2d3 f8c5 c2c3 d7d6'}], 'knodes': 68894779, 'depth': 63}\n",
      "{'pvs': [{'cp': 22, 'line': 'e2e4 e7e5 g1f3 b8c6 f1b5 g8f6 e1h1 f6e4 f1e1 e4d6'}, {'cp': 17, 'line': 'g1f3 d7d5 d2d4 e7e6 c2c4 g8f6 b1c3 f8b4 c4d5 e6d5'}, {'cp': 16, 'line': 'c2c4 e7e5 g2g3 g8f6 f1g2 f8c5 d2d3 e8h8 b1c3 f8e8'}, {'cp': 14, 'line': 'd2d4 g8f6 c2c4 e7e6 g2g3 d7d5 f1g2 f8b4 c1d2 b4e7'}, {'cp': 13, 'line': 'g2g3 c7c5 c2c4 g7g6 b1c3 b8c6 f1g2 f8g7 d2d3 d7d6'}], 'knodes': 97823307, 'depth': 60}\n",
      "{'pvs': [{'cp': 34, 'line': 'e2e4 e7e5 g1f3 b8c6 f1b5 g8f6 e1h1 f6e4 f1e1 e4d6'}, {'cp': 33, 'line': 'g1f3 g8f6 c2c4 e7e6 d2d4 d7d5 g2g3 f8b4 c1d2 b4e7'}, {'cp': 28, 'line': 'c2c4 g8f6 g1f3 e7e6 b1c3 d7d5 d2d4 f8b4 d1a4 b8c6'}, {'cp': 24, 'line': 'd2d4 g8f6 c2c4 e7e6 g1f3 d7d5 b1c3 b8d7 c1f4 f8b4'}, {'cp': 21, 'line': 'g2g3 d7d5 g1f3 g8f6 f1g2 g7g6 d2d4 f8g7 c2c4 c7c6'}, {'cp': 17, 'line': 'e2e3 d7d5 c2c4 e7e6 g1f3 g8f6 b2b3 f8e7 c1b2 e8h8'}, {'cp': 6, 'line': 'c2c3 d7d5 d2d4 g8f6 g1f3 c7c5 d4c5 e7e6 c1e3 f8e7'}, {'cp': 0, 'line': 'b1c3 d7d5 d2d4 g8f6 c1f4 e7e6 c3b5 b8a6 e2e3 f8e7'}, {'cp': -10, 'line': 'b2b3 e7e5 c1b2 b8c6 e2e3 g8f6 g1f3 e5e4 f3d4 f8c5'}, {'cp': -13, 'line': 'a2a3 c7c5 e2e4 g7g6 g1f3 f8g7 h2h3 g8f6 e4e5 f6g8'}], 'knodes': 744286, 'depth': 27}\n",
      "{'pvs': [{'cp': 40, 'line': 'g1f3 d7d5 d2d4 g8f6 c2c4 e7e6 g2g3 d5c4 f1g2 c7c5'}, {'cp': 39, 'line': 'c2c4 e7e5 g2g3 c7c6 d2d4 e5d4 d1d4 g8f6 f1g2 b8a6'}, {'cp': 37, 'line': 'd2d4 d7d5 g1f3 g8f6 c2c4 e7e6 b1c3 f8e7 c1f4 d5c4'}, {'cp': 36, 'line': 'e2e4 e7e6 d2d4 d7d5 b1c3 g8f6 c1g5 d5e4 c3e4 b8d7'}, {'cp': 31, 'line': 'g2g3 d7d5 g1f3 c7c5 f1g2 e7e6 d2d4 c5d4 f3d4 g8f6'}, {'cp': 10, 'line': 'e2e3 g8f6 d2d4 d7d5 c2c4 e7e6 g1f3 f8e7 f1e2 d5c4'}, {'cp': 10, 'line': 'c2c3 g8f6 g1f3 d7d5 d2d4 e7e6 c1f4 c7c5 e2e3 b8c6'}, {'cp': 4, 'line': 'b2b3 d7d5 g1f3 c8f5 c1b2 g8f6 f3h4 f5g4 h2h3 g4d7'}, {'cp': 1, 'line': 'b1c3 d7d5 d2d4 g8f6 c1f4 c7c5 e2e3 c5d4 e3d4 a7a6'}, {'cp': -7, 'line': 'd2d3 d7d5 e2e4 d5e4 d3e4 d8d1 e1d1 b8c6 b1d2 g8f6'}, {'cp': -8, 'line': 'a2a3 c7c5 c2c3 e7e6 d2d4 g8f6 g1f3 d7d5 c1f4 f8d6'}, {'cp': -13, 'line': 'a2a4 g8f6 d2d4 e7e6 g1f3 c7c5 e2e3 b8c6 c2c3 d7d5'}, {'cp': -16, 'line': 'h2h3 e7e5 e2e4 g8f6 b1c3 f8b4 a2a3 b4a5 g1f3 e8h8'}, {'cp': -23, 'line': 'b2b4 e7e5 c1b2 f8b4 b2e5 g8f6 c2c3 b4e7 e2e3 d7d5'}, {'cp': -29, 'line': 'f2f4 d7d5 g1f3 g8f6 e2e3 c8f5 f1e2 e7e6 e1h1 f8e7'}, {'cp': -62, 'line': 'g1h3 d7d5 g2g3 c7c5 f1g2 h7h5 d2d3 h5h4 b1d2 e7e5'}, {'cp': -63, 'line': 'h2h4 e7e5 c2c4 g8f6 g2g3 f8c5 f1g2 c7c6 e2e3 d7d5'}, {'cp': -85, 'line': 'b1a3 e7e5 d2d4 e5d4 d1d4 b8c6 d4a4 g8f6 g1f3 f8c5'}, {'cp': -97, 'line': 'f2f3 e7e5 e2e3 d7d5 d2d4 b8c6 f1b5 d8h4 g2g3 h4h5'}, {'cp': -155, 'line': 'g2g4 d7d5 f1g2 b8c6 c2c4 d5d4 b2b4 e7e5 d1a4 g8f6'}], 'knodes': 116945, 'depth': 21}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Each dataset element has a chess FEN position string (fen) and a list of evaluations (evals).\n",
    "Each eval has 3 fiels: pvs, knodes, depth.\n",
    "Each pvs (principal variations) can have more than 1 variation. Each variation has 2 fields: cp (centipawns), line (a list of moves).\n",
    "\"\"\"\n",
    "\n",
    "# Read the first json element and print it for the example\n",
    "file = open(PATH_DATASET_JSONL, \"r\")\n",
    "line = file.readline()\n",
    "element = json.loads(line)\n",
    "#print(json.dumps(element, indent=2))\n",
    "\n",
    "fen = element[\"fen\"]\n",
    "evals = element[\"evals\"]\n",
    "\n",
    "print(fen)\n",
    "print(\"len(evals):\", len(evals))\n",
    "for eval in evals:\n",
    "    print(eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting only the best line and its evaluation to create a simplified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_only_best_depth(path_dataset:str, path_output:str, verbose=False) -> None:\n",
    "    \"\"\"\n",
    "    Generate a new dataset with only the FEN position and the deepest evaluation of each one.\n",
    "    The resulting JSONL file has same number of lines as the original dataset.\n",
    "    Each line (element) has 6 fields: fen, cp, mate, depth, knodes, line.\n",
    "    The dataset is already sorted by depth and then by evaluation.\n",
    "    \"\"\"\n",
    "    dataset_positions_count = 0\n",
    "    with open(path_dataset, 'r') as infile, open(path_output, 'w') as outfile:\n",
    "        for idx, line in enumerate(infile):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                fen = data['fen']\n",
    "                evals = data['evals']\n",
    "                best_mate = None\n",
    "                best_cp = None\n",
    "\n",
    "                # Get the deepest evaluation (first in the list) and its principal variation (first in the list)\n",
    "                deepest_eval = evals[0]\n",
    "                pv = deepest_eval['pvs'][0]\n",
    "\n",
    "                # Get the best principal variation by mate or cp (first in the list)\n",
    "                if 'mate' in pv:\n",
    "                    best_mate = pv['mate']\n",
    "                if 'cp' in pv:\n",
    "                    best_cp = pv['cp']\n",
    "                               \n",
    "                # If there is no mate nor centipawns something is wrong\n",
    "                if best_mate is None and best_cp is None:\n",
    "                    raise ValueError('No mate or centipawns keys in the best eval')\n",
    "\n",
    "                # Prepare the output data\n",
    "                output_data = {\n",
    "                    \"fen\": fen,\n",
    "                    \"cp\": best_cp,\n",
    "                    \"mate\": best_mate,\n",
    "                    \"depth\": deepest_eval['depth'],\n",
    "                    \"knodes\": deepest_eval['knodes'],\n",
    "                    \"line\": pv['line']\n",
    "                }\n",
    "\n",
    "                # Write the output data as a JSON line\n",
    "                outfile.write(json.dumps(output_data) + '\\n')\n",
    "\n",
    "                dataset_positions_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in line {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Successfully generated the output dataset with\", dataset_positions_count, \"lines / positions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the function to generate the new dataset\n",
    "generate_dataset_only_best_depth(PATH_DATASET_JSONL, PATH_DATASET_JSONL_REDUCED, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the function used to map cp to win probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torre.utils import map_mate_or_cp_to_win_probability\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the matematical function\n",
    "cp = range(-1000, 1001)\n",
    "win = [map_mate_or_cp_to_win_probability(mate=None, cp=i) for i in cp]\n",
    "\n",
    "plt.plot(cp, win)\n",
    "plt.xlabel('Centipawns')\n",
    "plt.ylabel('Win probability')\n",
    "plt.title('Centipawns to win probability')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a JSONL dataset with FEN and win probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_with_fen_and_win_probability(path_dataset:str, path_output:str, verbose=False) -> None:\n",
    "    \"\"\"\n",
    "    Generate a new dataset with only the FEN position and the win probability of the best evaluation of each one.\n",
    "    The resulting JSONL file has same number of lines as the original dataset.\n",
    "    Each line (element) has 2 fields: fen, win_probability.\n",
    "    \"\"\"\n",
    "    dataset_positions_count = 0\n",
    "    with open(path_dataset, 'r') as infile, open(path_output, 'w') as outfile:\n",
    "        for idx, line in enumerate(infile):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                fen = data['fen']\n",
    "                cp = data['cp']\n",
    "                mate = data['mate']\n",
    "\n",
    "                # Map the mate or centipawns evaluation to win probability\n",
    "                win_probability = map_mate_or_cp_to_win_probability(mate, cp)\n",
    "                \n",
    "                # Prepare the output data\n",
    "                output_data = {\n",
    "                    \"fen\": fen,\n",
    "                    \"win_probability\": win_probability\n",
    "                }\n",
    "\n",
    "                # Write the output data as a JSONL line\n",
    "                outfile.write(json.dumps(output_data) + '\\n')\n",
    "\n",
    "                dataset_positions_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Error in line {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Successfully generated the output dataset with\", dataset_positions_count, \"lines / positions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create the JSONL file with the FEN position and the win probability\n",
    "create_dataset_with_fen_and_win_probability(PATH_DATASET_JSONL_REDUCED, PATH_DATASET_JSONL_REDUCED_WIN_PROB, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the new dataset into many files to reduce the single file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_jsonl_dataset(input_json_path:str, output_directory_path:str, max_lines_per_file:int=100_000, verbose=False) -> None:\n",
    "    \"\"\"\n",
    "    Split a JSON file into multiple JSON files with a maximum number of lines per file.\n",
    "    The resulting files are saves inside the output directory provided.\n",
    "    The resulting file names are the same as the original file with an index suffix.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_directory_path):\n",
    "        os.makedirs(output_directory_path)\n",
    "\n",
    "    with open(input_json_path, 'r') as infile:\n",
    "        data = infile.readlines()\n",
    "        num_files = len(data) // max_lines_per_file + 1\n",
    "\n",
    "        for i in range(num_files):\n",
    "            start = i * max_lines_per_file\n",
    "            end = start + max_lines_per_file\n",
    "            output_path = os.path.join(output_directory_path, f\"{i}.jsonl\")\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Writing file {output_path} with {len(data[start:end])} lines.\")\n",
    "\n",
    "            with open(output_path, 'w') as outfile:\n",
    "                outfile.writelines(data[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Split the JSONL file with the win probability dataset into multiple files\n",
    "split_jsonl_dataset(PATH_DATASET_JSONL_REDUCED_WIN_PROB, PATH_DIR_DATASET_JSONL_REDUCED_WIN_PROB_SPLIT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
